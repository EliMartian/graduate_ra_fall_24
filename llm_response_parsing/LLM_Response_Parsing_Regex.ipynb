{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Parsing\n",
        "\n",
        "**Motivation:** we want to be able to parse out the toxicity label (yes / no) and the associated toxicity category from poorly formed LLM JSON responses. And to make this more modular and re-usable, the following function is defined to handle different JSON files associated with different prompt instructions to the LLM. The resulting .csv output file contains correctly formatted and standardized data."
      ],
      "metadata": {
        "id": "Ft1CMesm7XYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knq1lCd97T-F"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Processes the LLM data to parse out the response into a standardized and processed\n",
        "# form to determine the overall toxic label for a Twitch chat message, and the associated\n",
        "# toxic categories for the chat (if any)\n",
        "def process_toxicity_data(input_csv, llama_json, zephyr_json, output_csv):\n",
        "    # Load the CSV into a DataFrame (uses encoding for formatting / punctuation errors)\n",
        "    df = pd.read_csv(input_csv, encoding='ISO-8859-1')\n",
        "\n",
        "    # Loads the JSON data from the corresponding Llama and Zephyr LLM response file\n",
        "    # Files are about ~200,000 rows in length\n",
        "    with open(llama_json, 'r') as f:\n",
        "        data_llama = json.load(f)\n",
        "\n",
        "    with open(zephyr_json, 'r') as f:\n",
        "        data_zephyr = json.load(f)\n",
        "\n",
        "    # Extracts the \"Is it toxic\" attribute from LLM responses\n",
        "    # (ie toxic label, but we use first attribute since the structure of json data can be malformed)\n",
        "    def extract_first_attribute(conversation):\n",
        "        assistant_content = conversation[2][\"content\"]\n",
        "\n",
        "        # Check if the content is malformed with nested 'content' field\n",
        "        if isinstance(assistant_content, dict) and 'content' in assistant_content:\n",
        "            nested_content = assistant_content['content']\n",
        "            # first try to filter on the is it toxic attribute that it was supposed to create in its LLM response\n",
        "            match_no = re.search(r'\"Is it toxic\": \"no\"', nested_content, re.IGNORECASE)\n",
        "            match_yes = re.search(r'\"Is it toxic\": \"yes\"', nested_content, re.IGNORECASE)\n",
        "            # otherwise, if the LLM response malformed, then search for the assistant piece\n",
        "            # where it contains its malformed response\n",
        "            match_malformed = re.search(r'<\\|assistant\\|>\\s*(.*)', nested_content, re.DOTALL)\n",
        "\n",
        "            if match_no:\n",
        "                return 'no'\n",
        "            elif match_yes:\n",
        "                return 'yes'\n",
        "            elif match_malformed:\n",
        "                body_response = match_malformed.group(1).lower()\n",
        "                if (\"non-toxic\" in body_response or\n",
        "                    \"not toxic\" in body_response or\n",
        "                    \"not flagged as toxic\" in body_response):\n",
        "                    return \"no\"\n",
        "                elif \"toxic\" in body_response:\n",
        "                    return \"yes\"\n",
        "                else:\n",
        "                    return 'none'\n",
        "        if isinstance(assistant_content, dict):\n",
        "            # if this is a map like we expect, we can parse out the toxic label easily\n",
        "            first_key = list(assistant_content.keys())[0]\n",
        "            return assistant_content[first_key]\n",
        "\n",
        "        return 'none'\n",
        "\n",
        "    # Extracts the second attribute from LLM responses\n",
        "    # (ie toxic category, but we use second attribute since the structure of the json data can be malformed)\n",
        "    def extract_second_attribute(conversation):\n",
        "        assistant_content = conversation[2][\"content\"]\n",
        "\n",
        "        if isinstance(assistant_content, dict) and 'content' in assistant_content:\n",
        "            nested_content = assistant_content['content']\n",
        "            match_no = re.search(r'\"Is it toxic\": \"no\"', nested_content, re.IGNORECASE)\n",
        "            match_malformed = re.search(r'<\\|assistant\\|>\\s*(.*)', nested_content, re.DOTALL)\n",
        "\n",
        "            if match_no:\n",
        "                return 'none'\n",
        "            elif match_malformed:\n",
        "                body_response = match_malformed.group(1).lower()\n",
        "                if (\"non-toxic\" in body_response or\n",
        "                    \"not toxic\" in body_response):\n",
        "                    return \"none\"\n",
        "\n",
        "                # define a list of categories to append to\n",
        "                categories = []\n",
        "                # some categories like obscen are misspelled intentionally, since the LLM\n",
        "                # can say obscene or obscentity, which differ in the character folloiwing 'n',\n",
        "                # hence the intentional misspell to capture the proper range of values\n",
        "                if \"insult\" in body_response:\n",
        "                    categories.append(\"insult\")\n",
        "                if \"obscen\" in body_response:\n",
        "                    categories.append(\"obscene\")\n",
        "                if \"sexual\" in body_response:\n",
        "                    categories.append(\"sexual_explicit\")\n",
        "                if \"identity\" in body_response:\n",
        "                    categories.append(\"identity_attack\")\n",
        "                if \"threat\" in body_response:\n",
        "                    categories.append(\"threat\")\n",
        "\n",
        "                return \", \".join(categories)\n",
        "\n",
        "        if isinstance(assistant_content, dict):\n",
        "            # if this is a map like we expect, we can parse out the toxic categories easily\n",
        "            keys = list(assistant_content.keys())\n",
        "            if len(keys) >= 2:\n",
        "                second_key = keys[1]\n",
        "                return assistant_content[second_key]\n",
        "\n",
        "        return 'none'\n",
        "\n",
        "    # Extract the first attribute values (ie toxic label, but we use first attribute since the structure of json data can be malformed)\n",
        "    first_attribute_values_llama = [extract_first_attribute(conversation) for conversation in data_llama]\n",
        "    first_attribute_values_zephyr = [extract_first_attribute(conversation) for conversation in data_zephyr]\n",
        "\n",
        "    # Extract the second attribute values (ie toxic category, but we use second attribute since the structure of the json data can be malformed)\n",
        "    second_attribute_values_llama = [extract_second_attribute(conversation) for conversation in data_llama]\n",
        "    second_attribute_values_zephyr = [extract_second_attribute(conversation) for conversation in data_zephyr]\n",
        "\n",
        "    toxic_df = pd.DataFrame()\n",
        "    toxic_df['Llama Label'] = first_attribute_values_llama\n",
        "    toxic_df['Llama Category'] = second_attribute_values_llama\n",
        "    toxic_df['Zephyr Label'] = first_attribute_values_zephyr\n",
        "    toxic_df['Zephyr Category'] = second_attribute_values_zephyr\n",
        "\n",
        "    # Normalize labels and categories, including things such as removing puncuation and ensuring lower case responses\n",
        "    # for standardization. Also, setting the LLM category or label to be none if it is not one that we support,\n",
        "    # which can be commented out depending on use case.\n",
        "    toxic_df['Zephyr Label'] = toxic_df['Zephyr Label'].str.lower().replace({'no': 'no', 'yes': 'yes'})\n",
        "    toxic_df['Llama Label'] = toxic_df['Llama Label'].str.lower().replace({'no': 'no', 'yes': 'yes'})\n",
        "    toxic_df['Zephyr Category'] = toxic_df['Zephyr Category'].str.replace(r\"['\\\"\\[\\]]\", '', regex=True).str.lower()\n",
        "    toxic_df['Llama Category'] = toxic_df['Llama Category'].str.replace(r\"['\\\"\\[\\]]\", '', regex=True).str.lower()\n",
        "\n",
        "    toxic_df.loc[~toxic_df['Zephyr Category'].isin(['insult', 'obscene', 'sexual_explicit', 'threat', 'identity_attack']), 'Zephyr Category'] = 'none'\n",
        "    toxic_df.loc[~toxic_df['Llama Category'].isin(['insult', 'obscene', 'sexual_explicit', 'threat', 'identity_attack']), 'Llama Category'] = 'none'\n",
        "    toxic_df.loc[~toxic_df['Zephyr Label'].isin(['yes', 'no', 'none']), 'Zephyr Label'] = 'none'\n",
        "    toxic_df.loc[~toxic_df['Llama Label'].isin(['yes', 'no', 'none']), 'Llama Label'] = 'none'\n",
        "\n",
        "    # Combine with the original DataFrame\n",
        "    df['Zephyr Vanilla_E Label'] = toxic_df['Zephyr Label']\n",
        "    df['Zephyr Vanilla_E Category'] = toxic_df['Zephyr Category']\n",
        "    df['Llama Vanilla_E Label'] = toxic_df['Llama Label']\n",
        "    df['Llama Vanilla_E Category'] = toxic_df['Llama Category']\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Processed data saved to '{output_csv}'.\")\n",
        "\n",
        "process_toxicity_data('toxicity_labels_full.csv', 'elias-llama-full-output-v_e.json', 'elias-zephyr-full-output-v_e.json', 'toxicity_labels_full_v_e.csv')\n",
        "process_toxicity_data('toxicity_labels_full_v_e.csv', 'elias-llama-full-output-v.json', 'elias-zephyr-full-output-v.json', 'toxicity_labels_full_v.csv')\n",
        "process_toxicity_data('toxicity_labels_full_v.csv', 'elias-llama-full-output-cot.json', 'elias-zephyr-full-output-cot.json', 'toxicity_labels_full_cot.csv')\n",
        "process_toxicity_data('toxicity_labels_full_cot.csv', 'elias-llama-full-output-cot_e.json', 'elias-zephyr-full-output-cot_e.json', 'toxicity_labels_full_cot_e.csv')\n"
      ]
    }
  ]
}